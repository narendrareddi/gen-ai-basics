# LangChain Runnables: Comprehensive Notes

These notes provide a concise yet comprehensive overview of **Runnables** in LangChain, a framework for building LLM-based applications. Runnables are standardized components that simplify the creation of flexible AI workflows by enabling seamless integration of various tasks. This guide covers the concept, purpose, types, and usage of Runnables, including syntax for invoking them, without relying on external context.

---

## What are Runnables?

Runnables are **standardized units of work** in LangChain that process an input to produce an output, following a common interface. They were introduced to address the issue of non-standardized components (e.g., PromptTemplate, LLMs, Retrievers), which previously required custom code to connect, leading to a complex codebase and steep learning curve.

### Key Characteristics of Runnables
1. **Unit of Work**: Each Runnable performs a specific task (e.g., formatting a prompt, invoking an LLM).
2. **Common Interface**: All Runnables implement standard methods, primarily `invoke`, ensuring consistent interaction. Other methods include `batch` (for multiple inputs) and `stream` (for streaming outputs).
3. **Connectability**: Runnables can be chained, where the output of one becomes the input for the next, enabling flexible workflows.
4. **Resulting Workflow as a Runnable**: A chain of Runnables is itself a Runnable, allowing nested and complex workflows.
5. **LEGO Analogy**: Runnables are like LEGO blocksâ€”each has a specific purpose, follows a standard connection mechanism, can be combined to form structures, and the resulting structure is itself connectable.

### Why Runnables?
- **Problem**: Early LangChain components (e.g., LLM, PromptTemplate) used different methods (e.g., `predict`, `format`), requiring custom chain functions (e.g., LLMChain, RetrievalQAChain) for each use case, bloating the codebase and complicating learning.
- **Solution**: Runnables standardize components by enforcing a common `invoke` interface, eliminating the need for custom chains, simplifying development, and enabling flexible workflows.

---

## Types of Runnables

Runnables are categorized into two main types:

1. **Task-Specific Runnables**:
   - Core LangChain components converted into Runnables (e.g., PromptTemplate, ChatOpenAI, Retrievers).
   - Each performs a specific function and can be chained with others.

2. **Runnable Primitives**:
   - Building blocks for orchestrating workflows by controlling how task-specific runnables are executed (e.g., sequentially, in parallel, or conditionally).
   - These primitives enable developers to create complex workflows with minimal code.

---

## Runnable Primitives

Below are the key **Runnable Primitives**, their purposes, and how to invoke them in LangChain using Python syntax. Each primitive is designed to orchestrate task-specific runnables effectively.

### 1. RunnableSequence
- **Purpose**: Executes runnables in a sequence, where the output of one runnable becomes the input for the next.
- **Use Case**: Generating a joke and then explaining it.
- **Syntax**:
  ```python
  from langchain_core.runnables import RunnableSequence

  chain = prompt | model | parser  # Using LCEL (LangChain Expression Language)
  # OR
  chain = RunnableSequence(prompt, model, parser)  # Explicit RunnableSequence
  result = chain.invoke({"topic": "AI"})  # Input dictionary for the first runnable
  ```
- **Explanation**: The `|` operator (LCEL) is a declarative syntax for creating a RunnableSequence, simplifying chain definition. The input is passed through each runnable in order.

### 2. RunnableParallel
- **Purpose**: Executes multiple runnables simultaneously with the same input, returning a dictionary of outputs.
- **Use Case**: Generating a tweet and a LinkedIn post for the same topic.
- **Syntax**:
  ```python
  from langchain_core.runnables import RunnableParallel

  parallel_chain = RunnableParallel(
      tweet=RunnableSequence(prompt1, model, parser),
      linkedin=RunnableSequence(prompt2, model, parser)
  )
  result = parallel_chain.invoke({"topic": "AI"})
  # Output: {"tweet": "...", "linkedin": "..."}
  ```
- **Explanation**: Each key in the dictionary (e.g., `tweet`, `linkedin`) maps to a runnable or chain, and all runnables receive the same input.

### 3. RunnablePassthrough
- **Purpose**: Passes the input unchanged to the output, useful for retaining intermediate results in a workflow.
- **Use Case**: Including the original joke alongside its explanation.
- **Syntax**:
  ```python
  from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableSequence

  chain = RunnableSequence(
      joke_chain,
      RunnableParallel(
          joke=RunnablePassthrough(),
          explanation=RunnableSequence(prompt, model, parser)
      )
  )
  result = chain.invoke({"topic": "cricket"})
  # Output: {"joke": "...", "explanation": "..."}
  ```
- **Explanation**: Often used within RunnableParallel to pass the input (e.g., a generated joke) to the output alongside other processed results.

### 4. RunnableLambda
- **Purpose**: Converts any Python function into a Runnable, allowing custom logic in workflows.
- **Use Case**: Counting words in a generated joke.
- **Syntax**:
  ```python
  from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnableSequence

  def word_count(text):
      return len(text.split())

  chain = RunnableSequence(
      joke_chain,
      RunnableParallel(
          joke=RunnablePassthrough(),
          word_count=RunnableLambda(word_count)
      )
  )
  result = chain.invoke({"topic": "AI"})
  # Output: {"joke": "...", "word_count": 10}
  ```
- **Explanation**: Wraps a Python function (e.g., `word_count`) into a Runnable, enabling it to be chained with other runnables.

### 5. RunnableBranch
- **Purpose**: Implements conditional logic, executing different runnables based on a condition.
- **Use Case**: Summarizing a report if it exceeds 500 words, otherwise passing it as-is.
- **Syntax**:
  ```python
  from langchain_core.runnables import RunnableBranch, RunnableSequence, RunnablePassthrough

  branch_chain = RunnableBranch(
      (lambda x: len(x.split()) > 500, RunnableSequence(prompt, model, parser)),
      RunnablePassthrough()
  )
  chain = RunnableSequence(report_chain, branch_chain)
  result = chain.invoke({"topic": "AI"})
  ```
- **Explanation**: Takes a tuple of (condition, runnable) pairs, with a default runnable (e.g., RunnablePassthrough) if no condition is met. The condition is a lambda function evaluating the input.

---

## LangChain Expression Language (LCEL)

- **Purpose**: A declarative syntax for defining sequential chains using the pipe (`|`) operator, simplifying the creation of RunnableSequences.
- **Benefit**: Makes chain definitions intuitive and readable, reducing boilerplate code.
- **Syntax**:
  ```python
  chain = prompt | model | parser
  result = chain.invoke({"topic": "AI"})
  ```
- **Current Scope**: Primarily supports sequential chains (RunnableSequence). Future versions may extend to other primitives (e.g., RunnableParallel, RunnableBranch).
- **Example**: Equivalent to `RunnableSequence(prompt, model, parser)` but more concise.

---

## Benefits of Runnables

1. **Standardization**: Ensures all components follow a common `invoke` interface, eliminating the need for custom chain functions.
2. **Flexibility**: Enables developers to chain runnables in various configurations (sequential, parallel, conditional) to create complex workflows.
3. **Simplicity**: Reduces codebase complexity and lowers the learning curve for new developers.
4. **Modularity**: Allows nested workflows, where chains of runnables are themselves runnables, supporting scalable application design.

---

## Key Takeaways

- **Runnables** are standardized units of work in LangChain, designed to process inputs and produce outputs using a common interface (primarily `invoke`).
- **Task-Specific Runnables** (e.g., PromptTemplate, ChatOpenAI) handle specific functions, while **Runnable Primitives** (Sequence, Parallel, Passthrough, Lambda, Branch) orchestrate workflows.
- **LCEL** simplifies sequential chain creation with a declarative pipe operator, with potential for broader application.
- Runnables make LangChain more intuitive, enabling developers to build complex LLM-based applications (e.g., RAG systems) with minimal code.

