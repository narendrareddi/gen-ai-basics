# LangChain Text Splitters: Comprehensive Notes

These notes provide a detailed overview of **Text Splitters** in LangChain, a critical component for building Retrieval-Augmented Generation (RAG) applications. Text Splitters break down large documents into smaller, manageable chunks for efficient processing by large language models (LLMs). This guide covers the concept, importance, types, usage, and code examples for Text Splitters, based on the provided transcript.

---

## What are Text Splitters?

Text Splitters are LangChain components that divide large text data (e.g., PDFs, articles, or HTML pages) into smaller chunks to facilitate processing by LLMs in RAG applications.

### Key Characteristics
1. **Purpose**: Break large text into smaller, manageable pieces (chunks) that LLMs can process effectively.
2. **Output**: A list of Document objects or strings, each representing a chunk with:
   - **Page Content**: The text content of the chunk.
   - **Metadata**: Information like source, chunk index, or other attributes inherited from the input Document.
3. **Key Parameters**:
   - **Chunk Size**: Defines the size of each chunk (e.g., in characters or tokens).
   - **Chunk Overlap**: Specifies overlapping characters or tokens between consecutive chunks to preserve context.
   - **Separator**: Defines boundaries for splitting (e.g., newlines, spaces, or custom separators).
4. **Process**: Split large text based on predefined strategies (e.g., length, structure, or semantics) to ensure compatibility with LLM context limits and improve output quality.

### Why Text Splitters?
- **Problem**: LLMs have context length limitations (e.g., 50,000 tokens), and processing large documents (e.g., a 1,000-page PDF) directly is inefficient and degrades output quality.
- **Solution**: Text Splitters divide large documents into smaller chunks, enabling:
  - Adherence to LLM context length limits.
  - Improved quality of embeddings, semantic search, and summarization.
  - Optimized computational resources through parallel processing and reduced memory usage.

---

## Importance of Text Splitting in RAG Applications

Text Splitting is crucial for LLM-powered applications due to the following reasons:

1. **Overcoming Model Limitations**:
   - **Context Length**: LLMs have a maximum input size (e.g., 50,000 tokens). Large documents often exceed this limit, making direct processing impossible.
   - **Example**: A PDF with 100,000+ words cannot be processed by an LLM with a 50,000-token limit. Text Splitters break it into manageable chunks (e.g., 500-word chunks).
   - **Benefit**: Enables processing of large documents by splitting them into smaller, processable units.

2. **Improved Downstream Task Performance**:
   - **Embedding Quality**: Smaller chunks capture semantic meaning more accurately than large texts. For example, splitting an article into paragraphs about different IPL teams (e.g., CSK, MI, RCB) creates embeddings that better represent each team’s context compared to embedding the entire article.
   - **Semantic Search**: Splitting improves search precision. For instance, searching for “Which IPL team does Virat Kohli play for?” yields better results when documents are split into team-specific chunks, as embeddings are more focused.
   - **Summarization**: LLMs produce higher-quality summaries with smaller chunks, avoiding drift or hallucination (e.g., generating irrelevant content).

3. **Optimized Computational Resources**:
   - **Memory Efficiency**: Smaller chunks require less memory than processing an entire document.
   - **Parallelization**: Chunks can be processed in parallel, reducing computation time.
   - **Example**: Processing a 1,000-page PDF as 1,000 smaller chunks is faster and less resource-intensive than handling the entire document.

---

## Types of Text Splitters

LangChain supports multiple Text Splitter strategies. This guide focuses on four key types: **Length-Based**, **Text Structure-Based**, **Document Structure-Based**, and **Semantic Meaning-Based**. Below are their details, including purpose, use cases, advantages, disadvantages, and Python syntax.

### 1. Length-Based Text Splitting
- **Purpose**: Splits text based on a fixed size (e.g., number of characters or tokens), ignoring linguistic structure.
- **Use Cases**: Simple scenarios where speed and simplicity are prioritized, such as splitting logs or basic text files.
- **Mechanism**:
  - Define a chunk size (e.g., 100 characters).
  - Traverse the text and split at the specified size, creating chunks.
  - Optionally include chunk overlap to preserve context between chunks.
- **Advantages**:
  - Simple and fast.
  - Easy to implement and computationally efficient.
- **Disadvantages**:
  - Ignores linguistic structure (e.g., grammar, sentences, paragraphs).
  - May split in the middle of words, sentences, or paragraphs, leading to loss of context.
  - Poor embedding quality for semantic tasks due to abrupt splits.
- **Syntax**:
  ```python
  from langchain_text_splitters import CharacterTextSplitter

  # Sample text
  text = "Space exploration has led to incredible scientific discoveries..."

  # Initialize splitter
  splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator="")
  chunks = splitter.split_text(text)  # Returns a list of strings
  print(len(chunks))  # Number of chunks
  print(chunks[0])  # First chunk
  ```
- **Example with Document Loader**:
  ```python
  from langchain_community.document_loaders import PyPDFLoader
  from langchain_text_splitters import CharacterTextSplitter

  # Load PDF
  loader = PyPDFLoader("dl_curriculum.pdf")
  docs = loader.load()  # List of Document objects

  # Initialize splitter
  splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
  chunks = splitter.split_documents(docs)  # Returns list of Document objects
  print(len(chunks))  # Number of chunks
  print(chunks[0].page_content)  # First chunk content
  print(chunks[0].metadata)  # Metadata (e.g., source, page)
  ```
- **Output Example**:
  ```python
  # Number of chunks: e.g., 50
  # First chunk: "Deep learning is a subset of machine learning that..."
  # Metadata: {'source': 'dl_curriculum.pdf', 'page': 0}
  ```
- **Notes**:
  - **Chunk Overlap**: Overlapping characters between chunks (e.g., 10–20% of chunk size) preserve context, mitigating abrupt cuts. For a 100-character chunk, a 10–20 character overlap is recommended.
  - **Use Case Limitation**: Best for non-semantic tasks; less effective for embeddings or semantic search due to context loss.

### 2. Text Structure-Based Text Splitting
- **Purpose**: Splits text based on its linguistic structure (e.g., paragraphs, sentences, words) using a recursive approach to preserve context.
- **Use Cases**: Processing natural language texts (e.g., articles, books) where maintaining sentence or paragraph boundaries improves semantic quality.
- **Mechanism**:
  - Uses a hierarchy of separators: paragraphs (`\n\n`), sentences (`\n`), words (space), and characters.
  - Recursively tries to split at higher levels (e.g., paragraphs) first. If chunks exceed the size limit, it moves to lower levels (e.g., sentences, words, characters).
  - Attempts to merge smaller chunks to optimize size while respecting the chunk size limit.
- **Advantages**:
  - Preserves linguistic structure (e.g., avoids splitting mid-sentence).
  - Better for semantic tasks like embeddings and search due to context retention.
  - Widely used in RAG applications.
- **Disadvantages**:
  - More complex than length-based splitting.
  - Slightly slower due to recursive processing.
- **Syntax**:
  ```python
  from langchain_text_splitters import RecursiveCharacterTextSplitter

  text = "My name is Nitish. I am 35 years old.\n\nI live in Gurgaon.\nHow are you?"

  # Initialize splitter
  splitter = RecursiveCharacterTextSplitter(chunk_size=25, chunk_overlap=0)
  chunks = splitter.split_text(text)
  print(len(chunks))  # Number of chunks
  print(chunks)  # List of chunks
  ```
- **Example Output** (Chunk Size = 25):
  ```python
  # Number of chunks: 4
  # Chunks: ['My name is Nitish.', 'I am 35 years old.', 'I live in Gurgaon.', 'How are you?']
  ```
- **Example with Larger Chunk Size** (Chunk Size = 50):
  ```python
  # Number of chunks: 2
  # Chunks: ['My name is Nitish. I am 35 years old.', 'I live in Gurgaon.\nHow are you?']
  ```
- **Notes**:
  - **Separators**: Default separators are `["\n\n", "\n", " ", ""]`. Custom separators can be defined.
  - **Optimization**: Merges smaller chunks to approach the chunk size limit, ensuring meaningful splits.
  - **Use Case**: Ideal for most RAG applications due to its balance of simplicity and context preservation.

### 3. Document Structure-Based Text Splitting
- **Purpose**: Extends text structure-based splitting to specialized document formats (e.g., code, Markdown, HTML) using format-specific separators.
- **Use Cases**: Processing structured documents like Python code (split by `class`, `def`), Markdown (split by headings, lists), or HTML (split by tags).
- **Mechanism**:
  - Uses RecursiveCharacterTextSplitter with format-specific separators (e.g., `class`, `def` for Python; `#`, `##` for Markdown).
  - Follows the same recursive approach but prioritizes document-specific constructs over generic linguistic structures.
- **Advantages**:
  - Tailored to structured documents, preserving their logical organization.
  - Improves embedding and search quality for non-plain-text formats.
- **Disadvantages**:
  - Requires knowledge of the document format to define appropriate separators.
  - Limited to supported formats (e.g., Python, Markdown, HTML).
- **Syntax** (Python Code):
  ```python
  from langchain_text_splitters import RecursiveCharacterTextSplitter, Language

  python_code = """
  class MyClass:
      def __init__(self):
          self.name = "Nitish"
      def greet(self):
          print("Hello")
  obj = MyClass()
  if True:
      obj.greet()
  """

  # Initialize splitter for Python
  splitter = RecursiveCharacterTextSplitter.from_language(
      language=Language.PYTHON, chunk_size=50, chunk_overlap=0
  )
  chunks = splitter.split_text(python_code)
  print(len(chunks))  # Number of chunks
  print(chunks[0])  # First chunk
  ```
- **Output Example**:
  ```python
  # Number of chunks: 3
  # First chunk: "class MyClass:\n    def __init__(self):\n        self.name = \"Nitish\""
  ```
- **Syntax** (Markdown):
  ```python
  markdown_text = """
  # Introduction
  Welcome to my project.
  ## Features
  - Fast
  - Reliable
  """

  splitter = RecursiveCharacterTextSplitter.from_language(
      language=Language.MARKDOWN, chunk_size=50, chunk_overlap=0
  )
  chunks = splitter.split_text(markdown_text)
  print(len(chunks))  # Number of chunks
  print(chunks)  # List of chunks
  ```
- **Output Example**:
  ```python
  # Number of chunks: 3
  # Chunks: ['# Introduction\nWelcome to my project.', '## Features', '- Fast\n- Reliable']
  ```
- **Notes**:
  - **Supported Languages**: Python, JavaScript, Java, PHP, Markdown, HTML, and more.
  - **Separators**: Language-specific (e.g., `class`, `def` for Python; `#`, `##` for Markdown).
  - **Use Case**: Essential for processing code or structured markup in RAG applications.

### 4. Semantic Meaning-Based Text Splitting
- **Purpose**: Splits text based on semantic similarity, grouping sentences with similar meanings into the same chunk.
- **Use Cases**: Scenarios where topics change within a paragraph, requiring splits based on meaning rather than structure (e.g., a paragraph discussing agriculture and IPL).
- **Mechanism**:
  - Splits text into sentences.
  - Generates embeddings for each sentence using an embedding model (e.g., OpenAI).
  - Computes similarity (e.g., cosine similarity) between consecutive sentence embeddings.
  - Splits at points where similarity drops significantly (e.g., below a threshold based on standard deviation).
- **Advantages**:
  - Captures semantic context, ideal for documents with mixed topics.
  - Improves embedding and search quality by grouping related content.
- **Disadvantages**:
  - Experimental and less reliable in current LangChain implementations.
  - Computationally expensive due to embedding generation and similarity calculations.
  - Requires fine-tuning thresholds for optimal results.
- **Syntax**:
  ```python
  from langchain_experimental.text_splitter import SemanticChunker
  from langchain_openai import OpenAIEmbeddings

  text = """
  Farmers are adopting new techniques in agriculture. The sun was bright and the air smelled of earth and fresh grass.
  The Indian Premier League is a popular cricket tournament. Terrorism remains a global challenge.
  """

  # Initialize splitter
  splitter = SemanticChunker(
      embeddings=OpenAIEmbeddings(),
      breakpoint_threshold_type="standard_deviation",
      breakpoint_threshold_amount=1
  )
  chunks = splitter.split_text(text)
  print(len(chunks))  # Number of chunks
  print(chunks)  # List of chunks
  ```
- **Output Example**:
  ```python
  # Number of chunks: 3
  # Chunks: [
  #   "Farmers are adopting new techniques in agriculture. The sun was bright and the air smelled of earth and fresh grass.",
  #   "The Indian Premier League is a popular cricket tournament.",
  #   "Terrorism remains a global challenge."
  # ]
  ```
- **Notes**:
  - **Threshold Types**: Standard deviation, percentile, interquartile, or gradient. Standard deviation is commonly used.
  - **Threshold Tuning**: Adjust `breakpoint_threshold_amount` (e.g., 1, 2, or 3 standard deviations) to control split sensitivity.
  - **Limitations**: Experimental in LangChain, with inconsistent performance. RecursiveCharacterTextSplitter is often preferred.

---

## Chunk Overlap

- **Definition**: The number of characters or tokens shared between consecutive chunks to preserve context.
- **Purpose**: Prevents loss of context due to abrupt splits, especially in length-based splitting.
- **Example**:
  - Text: "Space exploration has led to incredible scientific discoveries..."
  - Chunk Size: 100 characters, Chunk Overlap: 10 characters.
  - Chunk 1: "Space exploration has led to incredible scientific discoveries..."
  - Chunk 2: Starts from the last 10 characters of Chunk 1, ensuring continuity.
- **Benefits**:
  - Retains context across chunks, improving embedding and search quality.
  - Mitigates issues with mid-word or mid-sentence splits.
- **Trade-Off**:
  - Higher overlap increases the number of chunks, raising computational costs.
  - Recommended overlap: 10–20% of chunk size (e.g., 10–20 characters for a 100-character chunk).
- **Syntax Example**:
  ```python
  splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)
  chunks = splitter.split_text(text)
  ```

---

## Key Takeaways
- **Text Splitters** are essential for breaking large documents into smaller chunks for RAG applications, addressing LLM context limits and improving output quality.
- **Core Types**:
  - **Length-Based**: Splits by fixed size (e.g., characters); simple but ignores context.
  - **Text Structure-Based**: Splits by linguistic structure (e.g., paragraphs, sentences); widely used for natural language.
  - **Document Structure-Based**: Splits by format-specific constructs (e.g., code, Markdown); ideal for structured documents.
  - **Semantic Meaning-Based**: Splits by semantic similarity; experimental but promising for mixed-topic texts.
- **Chunk Overlap**: Preserves context by including shared content between chunks; 10–20% of chunk size is recommended.
- **Best Practice**: RecursiveCharacterTextSplitter is the most commonly used due to its balance of simplicity and context preservation.
- **Integration with Document Loaders**: Text Splitters work seamlessly with Document Loaders to process loaded documents into chunks for downstream tasks (e.g., embeddings, retrieval).
