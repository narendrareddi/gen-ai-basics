# LangChain Prompts Component: Comprehensive Notes

These notes summarize the key concepts and important information about the **Prompts Component** in LangChain, focusing on static and dynamic prompts, prompt templates, chat prompt templates, message types, and message placeholders.

---

## 1. Introduction to Prompts in LangChain

The **Prompts Component** in LangChain is a critical part of the framework, enabling developers to design and manage inputs sent to language models (LLMs). Prompts determine the quality and relevance of the LLM's output, making them a cornerstone of effective LLM interactions.

### Key Points:
- **Definition**: A prompt is the input (text, image, or other data) sent to an LLM to elicit a response.
- **Importance**: The LLM's output is highly sensitive to the prompt's wording, structure, and context.
- **Types of Prompts**:
  1. **Static Prompts**: Fixed prompts with no dynamic elements.
  2. **Dynamic Prompts**: Prompts with placeholders that are filled at runtime.
- **Focus**: This video primarily covers **text-based prompts**, as they are the most common in current applications.
- **Prompt Engineering**: A specialized field focused on crafting effective prompts to optimize LLM performance.

---

## 2. Recap of Previous Video (Models Component)

- **Models Covered**: Language Models (LLMs and Chat Models) and Embedding Models.
- **Key Correction on Temperature Parameter**:
  - **Incorrect Explanation (Previous Video)**: Temperature range stated as 0 to 2, with 0 being deterministic and 2 being creative.
  - **Correct Explanation**: 
    - Temperature typically ranges from **0.0 to 1.5+** (varies by model).
    - **0.0**: Deterministic output (same input yields same output every time).
    - **1.5+**: Creative output (same input yields varied outputs).
    - **Use Cases**:
      - Low temperature (0.0–0.3): Factual, consistent tasks (e.g., answering factual queries).
      - High temperature (0.9–1.5): Creative tasks (e.g., poetry, storytelling).
  - **Example Insight**: For an application requiring consistent outputs (e.g., factual answers), set temperature close to 0. For varied, creative outputs, set it higher (e.g., 1.5).

---

## 3. Static vs. Dynamic Prompts

### 3.1. Static Prompts
- **Definition**: Fixed prompts written directly in the code or provided by the user without placeholders.
- **Example**: `Write a five-line poem on cricket`.
- **Characteristics**:
  - Sent directly to the LLM using the `invoke` method.
  - No runtime modifications.
- **Issues**:
  - Lack of flexibility: Users must write the entire prompt, increasing the risk of errors (e.g., misspelled paper titles in a research summarization tool).
  - Inconsistent user experience: Variations in prompt wording can lead to vastly different LLM outputs.
  - Not scalable for real-world applications where user inputs vary.
- **Use Case**: Suitable for one-off, simple queries but not recommended for production applications.

### 3.2. Dynamic Prompts
- **Definition**: Prompts with placeholders that are filled dynamically at runtime based on user input or other variables.
- **Example**: A template like `Please summarize the research paper titled {paper_input} in {style} style with {length} length` where `{paper_input}`, `{style}`, and `{length}` are filled dynamically.
- **Advantages**:
  - Ensures consistent user experience by standardizing prompt structure.
  - Reduces errors (e.g., by providing dropdowns for paper titles).
  - Scalable for real-world applications.
- **Use Case**: Ideal for applications like research assistants, where users provide specific inputs (e.g., paper title, style, length) to generate tailored outputs.

---

## 4. Prompt Template in LangChain

The `PromptTemplate` class in LangChain is used to create dynamic prompts for single-turn interactions.

### Key Features:
- **Purpose**: Allows creation of reusable templates with placeholders.
- **Validation**: Automatically validates that all placeholders are filled, reducing runtime errors.
- **Reusability**: Templates can be saved as JSON files and reused across different parts of an application.
- **Integration**: Tightly coupled with LangChain’s ecosystem (e.g., chains for combining prompts with models).

### Example Workflow:
1. Define a template with placeholders (e.g., `{paper_input}`, `{style}`, `{length}`).
2. Fill placeholders at runtime using user inputs.
3. Pass the filled prompt to the LLM.

#### Implementation Notes:
- **Class**: `PromptTemplate` from `langchain_core.prompts`.
- **Key Methods**:
  - `invoke`: Fills placeholders and generates the final prompt.
  - `save`: Saves the template as a JSON file for reusability.
  - `load_prompt`: Loads a saved template from a JSON file.
- **Validation**: Use `validate_template=True` to ensure all placeholders are provided, throwing an error if any are missing.

### Benefits Over f-strings:
1. **Built-in Validation**: Ensures all placeholders are filled, catching errors during development.
2. **Reusability**: Templates can be saved and loaded across multiple files or pages.
3. **Ecosystem Integration**: Works seamlessly with LangChain’s chains and other components.

---

## 5. Building a Research Assistant UI with Dynamic Prompts

### Objective:
Create a web-based research assistant tool where users can summarize research papers by selecting:
- Paper title (from a dropdown to avoid errors).
- Explanation style (e.g., simple, code-heavy, math-heavy).
- Summary length (e.g., short, medium, long).

### Workflow:
1. **UI Setup** (using Streamlit):
   - Create dropdowns for paper title, style, and length.
   - Add a "Summarize" button to trigger the LLM.
2. **Prompt Template**:
   - Define a template with placeholders for `{paper_input}`, `{style}`, and `{length}`.
   - Include specifications for mathematical details, analogies, and clarity.
3. **Processing**:
   - Fetch user inputs from dropdowns.
   - Fill the template using `PromptTemplate.invoke`.
   - Pass the filled prompt to the LLM and display the result.

### Key Insights:
- **Dynamic Prompts**: Ensure consistency by limiting user control to specific inputs (e.g., dropdowns).
- **Error Prevention**: Dropdowns eliminate issues like misspelled paper titles.
- **Scalability**: The template can be reused for different papers and styles.

---

## 6. Chat Prompt Template in LangChain

The `ChatPromptTemplate` class is used for dynamic prompts in multi-turn conversations, where a list of messages is sent to the LLM.

### Key Features:
- **Purpose**: Creates templates for a sequence of messages (e.g., system, human, AI messages).
- **Use Case**: Ideal for chatbots requiring dynamic system or human messages.
- **Syntax**: Similar to `PromptTemplate`, but designed for lists of messages.

### Example Scenario:
- **System Message**: `You are a helpful {domain} expert`.
- **Human Message**: `Explain in simple terms what is {topic}`.
- **Implementation**: Use `ChatPromptTemplate` to define placeholders for `{domain}` and `{topic}`, filled dynamically at runtime.

#### Implementation Notes:
- **Class**: `ChatPromptTemplate` from `langchain_core.prompts`.
- **Syntax Options**:
  - Use tuples to specify message roles and content: `[("system", "You are a helpful {domain} expert"), ("human", "Explain {topic}")]`.
  - Alternatively, use `ChatPromptTemplate.from_messages` for similar functionality.
- **Recommendation**: Use the tuple-based syntax, as it aligns with the latest LangChain documentation.

---

## 7. Building a Simple Chatbot

### Objective:
Create a console-based chatbot that maintains conversation history and supports multi-turn interactions.

### Workflow:
1. **Setup**:
   - Initialize a `ChatOpenAI` model (or open-source alternative like Hugging Face).
   - Create an empty list for chat history.
2. **Conversation Loop**:
   - Use a `while True` loop to continuously prompt the user for input.
   - Check if the input is "exit" to break the loop.
   - Convert user input to a `HumanMessage`.
   - Append the `HumanMessage` to the chat history.
   - Send the entire chat history to the LLM using `invoke`.
   - Convert the LLM’s response to an `AIMessage` and append it to the chat history.
   - Print the response with an "AI" label.
3. **Output**: Display the full chat history when the user exits.

### Key Issue:
- Without proper labeling, the chatbot cannot distinguish between user and AI messages, leading to context loss.
- **Solution**: Use LangChain’s message types to label messages explicitly.

---

## 8. Message Types in LangChain

LangChain supports three types of messages to maintain context in conversations:

1. **System Message**:
   - **Purpose**: Sets the context or role for the LLM at the start of a conversation (e.g., `You are a helpful assistant`).
   - **Placement**: Typically the first message in the chat history.
   - **Use Case**: Defines the LLM’s behavior or expertise (e.g., doctor, teacher).

2. **Human Message**:
   - **Purpose**: Represents user input sent to the LLM (e.g., `Tell me about LangChain`).
   - **Use Case**: Captures user queries or instructions.

3. **AI Message**:
   - **Purpose**: Represents the LLM’s response (e.g., `LangChain is a framework for building LLM applications`).
   - **Use Case**: Stores the LLM’s output in the chat history.

### Implementation Notes:
- **Classes**: `SystemMessage`, `HumanMessage`, `AIMessage` from `langchain_core.messages`.
- **Benefits**:
  - Clear labeling ensures the LLM understands the conversation flow.
  - Prevents confusion as chat history grows.
- **Example**:
  - Chat history: `[SystemMessage("You are a helpful assistant"), HumanMessage("Tell me about LangChain"), AIMessage("LangChain is...")]`.

---

## 9. Fixing the Chatbot with Message Types

### Issue with Basic Chatbot:
- The chatbot stores messages without labeling (user vs. AI), causing context loss in multi-turn conversations.
- Example: If a user asks, “Multiply the bigger number by 10” after comparing 2 and 0, the LLM may not know the bigger number is 2 without context.

### Solution:
- Maintain a labeled chat history using `SystemMessage`, `HumanMessage`, and `AIMessage`.
- **Updated Workflow**:
  1. Initialize chat history with a `SystemMessage` (e.g., `You are a helpful AI assistant`).
  2. Convert user input to a `HumanMessage` and append to chat history.
  3. Send the entire chat history to the LLM.
  4. Convert the LLM’s response to an `AIMessage` and append to chat history.
  5. Print the response and store the full history.

### Benefits:
- The LLM understands the conversation context, improving response accuracy.
- Labeled messages ensure clarity, even in long conversations.

---

## 10. Message Placeholder in LangChain

### Definition:
A `MessagePlaceholder` is a special placeholder in a `ChatPromptTemplate` used to dynamically insert a list of messages (e.g., chat history) at runtime.

### Use Case:
- **Scenario**: A customer support chatbot where users ask about order statuses (e.g., refunds) across multiple sessions.
- **Requirement**: Load previous chat history to provide context for new queries (e.g., “Where is my refund?” requires knowing a prior refund request).

### Implementation Notes:
- **Class**: `MessagesPlaceholder` from `langchain_core.prompts`.
- **Workflow**:
  1. Create a `ChatPromptTemplate` with a `MessagesPlaceholder` for chat history.
  2. Load past chat history (e.g., from a database or text file).
  3. Fill the placeholder with the chat history and the current user query.
  4. Pass the resulting prompt to the LLM.

#### Example Workflow:
- **Chat History**: 
  - Human: `I want to request a refund for order #12345`.
  - AI: `Your refund request has been initiated. Expect it in 3-5 business days`.
- **New Query**: `Where is my refund?`.
- **Template**: `[SystemMessage("You are a helpful customer support agent"), MessagesPlaceholder("chat_history"), HumanMessage("{query}")]`.
- **Execution**:
  - Load chat history into `MessagesPlaceholder`.
  - Fill `{query}` with `Where is my refund?`.
  - Send to LLM, which uses the history to provide context-aware responses.

### Key Benefits:
- Simplifies management of chat history in multi-session conversations.
- Ensures context retention for accurate responses.

---

## 11. Logical Diagram of Prompt Usage

### Single-Turn Queries:
- **Method**: Use `model.invoke` with a single message.
- **Prompt Types**:
  - **Static**: Direct prompt (e.g., `Write a poem on cricket`).
  - **Dynamic**: Use `PromptTemplate` with placeholders (e.g., `{topic}`).
- **Use Case**: One-off tasks like summarizing a research paper.

### Multi-Turn Conversations:
- **Method**: Use `model.invoke` with a list of messages.
- **Prompt Types**:
  - **Static**: List of labeled messages (`SystemMessage`, `HumanMessage`, `AIMessage`).
  - **Dynamic**: Use `ChatPromptTemplate` with placeholders for dynamic system or human messages.
- **Use Case**: Chatbots requiring conversation history.

### Message Placeholder:
- **Purpose**: Dynamically insert chat history into a `ChatPromptTemplate`.
- **Use Case**: Customer support bots needing past context.

---

## 12. Key Takeaways
- **Prompts**: Critical for controlling LLM output; small changes can significantly alter responses.
- **Static vs. Dynamic Prompts**:
  - Static prompts are inflexible and error-prone in real-world applications.
  - Dynamic prompts use templates for consistency and scalability.
- **PromptTemplate**: Used for single-turn dynamic prompts with validation and reusability.
- **ChatPromptTemplate**: Used for multi-turn conversations with dynamic message sequences.
- **Message Types**: `SystemMessage`, `HumanMessage`, and `AIMessage` ensure clear context in conversations.
- **MessagePlaceholder**: Enables dynamic insertion of chat history for context-aware responses.
